{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install humanize\n",
    "#! pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd366d",
   "metadata": {},
   "source": [
    "# Label noise\n",
    "\n",
    "\n",
    "## Problem statement \n",
    "Have some binary classification task, traditionally assume data of the form X,y\n",
    "\n",
    "In reality, some of the labels may be incorrect, distinguish\n",
    "```\n",
    "y - true label\n",
    "y* - observed, possibly incorrect label\n",
    "```\n",
    "\n",
    "This can obviously effect model training, validation. Would also effect benchmarking process (comparing performance on noisy data doesn't tell you about performance on actual data).\n",
    "\n",
    "## Types of noise\n",
    "\n",
    "Can be completely independent:\n",
    "`p(y* != y | x, y) = p(y* != y)`\n",
    "\n",
    "class-dependent, depends on y:\n",
    "`p(y* != y | x, y) = p(y* != y | y)`\n",
    "\n",
    "feature-dependent, depends on x:\n",
    "`p(y* != y | x, y) = p(y* != y | x, y)`\n",
    "\n",
    "In fraud modeling, higher likelihood of `(y*, y) = (0, 1)` than reverse.\n",
    "(missed fraud, label maturity, intentional data poisoning, etc.)\n",
    "\n",
    "\"feature-dependent\" is probably most realistic in fraud but fewer removal techniques and also harder to synthetically generate. We will work with \"boundary conditional\" noise, probability of being mislabeled is weighted by distance from some decision boundary (score from model trained on clean data), implemented in scikit-clean.\n",
    "\n",
    "## Literature/packages\n",
    "\n",
    "Many methods in the literature to address this; can build loss functions that are robust to noise, can try to identify and filter (remove) or clean (flip label) examples identified as noisy.\n",
    "\n",
    "Some packages including CleanLab and scikit-clean. Can also hand-code an ensemble method. Most of these are model-agnostic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b172deb",
   "metadata": {},
   "source": [
    "## CleanLab\n",
    "\n",
    "well-established, state of the art, open source package with some theoretical guarantees\n",
    "\n",
    "score all examples with y* = 1, determine average score t_1\n",
    "now score all examples with y* = 0. Any that score above t_1 are marked as noise\n",
    "\n",
    "can wrap any (sklearn-compatible) model with this process. \n",
    "\n",
    "## scikit-clean \n",
    "\n",
    "library of several different approaches including filtering as well as noise generation. Is similarly designed to be model-agnostic but doesn't always do a great job (doesn't handle unencoded categorical features well). Some of its methods can also be *very* slow relative to others\n",
    "\n",
    "## micro-models\n",
    "\n",
    "slice up training data, train a model on each slice, let models vote on whether to remove data. Can use majority (more than half of models \"misclassify\" example), consensus (all models misclassify) or any other threshold.\n",
    "\n",
    "## experiment design\n",
    "\n",
    "take 7 of the datasets - [‘ieeecis’, ‘ccfraud’, ‘fraudecom’, ‘sparknov’, ‘fakejob’, ‘vehicleloan’,‘twitterbot’]\n",
    "* drop IP and malurl dataset as they are difficult to work with \"out of the box\"\n",
    "* use numerical and categorical features, target-encode categorical features (drop text and enrichable features)\n",
    "\n",
    "add boundary-conditional noise `n` to training data (flipping both classes).\n",
    "\n",
    "values: `n in [0, 0.1, 0.2, 0.3, 0.4, 0.5]`\n",
    "    \n",
    "target encoding is done after noise is added\n",
    "    \n",
    "Catboost used as base classifier in all cases (with default settings)\n",
    "\n",
    "compare following methods for cleaning training data\n",
    "* baseline (no cleaning done)\n",
    "* CleanLab\n",
    "* scikit-clean MCS \n",
    "* micro-model majority voting (hand-built)\n",
    "* micro-model consensus voting (hand-built)\n",
    "\n",
    "measure AUC on (clean) test data\n",
    "\n",
    "repeat process 5 times for each experiment (start with clean data, add random noise, filter noise back out, train classifier, etc.), compute mean and std. dev of AUC for each\n",
    "\n",
    "CleanLab usually winds up being the best, but not uniformly. Baseline is sometimes the best for zero noise (as expected), and sometimes MCS or micro-model majority will come out ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import humanize\n",
    "import pickle\n",
    "\n",
    "# basics from sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "# noise generation\n",
    "from skclean.simulate_noise import flip_labels_cc, BCNoise\n",
    "\n",
    "# base classifiers\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# cleaning methods/helpers\n",
    "from cleanlab.classification import CleanLearning\n",
    "from micro_models import MicroModelCleaner\n",
    "from skclean.pipeline import Pipeline\n",
    "from skclean.handlers import Filter\n",
    "from skclean.detectors import MCS\n",
    "\n",
    "# dataset loader\n",
    "from load_fdb_datasets import prepare_noisy_dataset, dataset_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85117ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper definitions for the various types of cleaning methods we will use. \n",
    "# Each one wraps a model_class (in our case catboost, but could use xgboost, etc.)\n",
    "# resulting model_class can then take noisy data in its .fit() method and clean before training\n",
    "\n",
    "def baseline_model(model_class, params):\n",
    "    return model_class(**params)\n",
    "\n",
    "def cleanlab_model(model_class, params, pulearning=False):\n",
    "    if pulearning:\n",
    "        return CleanLearning(model_class(**params), pulearning=pulearning)\n",
    "    else:\n",
    "        return CleanLearning(model_class(**params))\n",
    "    \n",
    "def micromodels(model_class, pulearning, num_clfs, threshold, params):\n",
    "    return MicroModelCleaner(model_class, pulearning=pulearning, num_clfs=num_clfs, threshold=threshold, **params)\n",
    "\n",
    "def skclean_MCS(model_class, params):\n",
    "    skclean_pipeline = Pipeline([\n",
    "        ('detector',MCS(classifier=model_class(**params))),\n",
    "        ('handler',Filter(model_class(**params)))\n",
    "    ])\n",
    "    return skclean_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bcd08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some high-level parameters, \n",
    "# the number of runs for each experiment (determine mean/std. dev)\n",
    "num_samples = 5 \n",
    "# whether to use target encoding on categorical features\n",
    "target_encoding = True\n",
    "# whether to save intermediate results to disk (in case of failure etc.)\n",
    "save_results = True\n",
    "\n",
    "# we will be creating a lot of classifiers, let's use the same parameters for each\n",
    "model_config_dict = {\n",
    "    'catboost': {\n",
    "        'model_class': CatBoostClassifier,\n",
    "        'default_params': {\n",
    "            'verbose': False,\n",
    "            'iterations': 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# all of our experiments will use catboost and boundary-consistent noise\n",
    "base_model_type = 'catboost'\n",
    "noise_type = 'boundary-consistent'\n",
    "model_class = model_config_dict[base_model_type]['model_class']\n",
    "\n",
    "# the set of experimental parameters, we will iterate over all these datasets\n",
    "keys = ['ieeecis', 'sparknov', 'ccfraud', 'fraudecom', 'fakejob', 'vehicleloan', 'twitterbot']\n",
    "# all these cleaning methods\n",
    "clf_types = ['baseline', 'skclean_MCS', 'cleanlab', 'micromodels_majority', 'micromodels_consensus']\n",
    "# all these noise levels\n",
    "noise_amounts = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# and we will let cleaning methods know that noise can happen for either class\n",
    "pulearning = None\n",
    "\n",
    "# a little bit of setup for saving intermediate results to disk\n",
    "if save_results:\n",
    "    results_file_path = './results'\n",
    "    results_file_name = '{}_noise_benchmark_results.pkl'\n",
    "    try:\n",
    "        os.mkdir(results_file_path)\n",
    "    except OSError as error:\n",
    "        print(error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize results dict, we will index results by dataset/noise_amount/cleaning_method\n",
    "results = {}\n",
    "\n",
    "# main experimental loop   \n",
    "for key in keys:\n",
    "    # check to see if we have already run this experiment and saved to disk\n",
    "    full_result_path = os.path.join(results_file_path,results_file_name.format(key))\n",
    "    if os.path.exists(full_result_path) and save_results:\n",
    "        with open(full_result_path, 'rb') as results_file:\n",
    "            results[key] = pickle.load(results_file)\n",
    "    # otherwise start from scratch\n",
    "    else:\n",
    "        # initialize sub-results\n",
    "        results[key] = {}\n",
    "        model_params = model_config_dict[base_model_type]['default_params']\n",
    "        \n",
    "        for noise_amount in noise_amounts:\n",
    "            print(f\"\\n =={key}_{noise_amount}== \\n\")\n",
    "            \n",
    "            # initialize sub-sub-results\n",
    "            results[key][noise_amount] = {}\n",
    "\n",
    "            # these are the cleaning classifiers we will use\n",
    "            clfs = {\n",
    "                'baseline': baseline_model(model_class, model_params),\n",
    "                'skclean_MCS': skclean_MCS(model_class, model_params),\n",
    "                'cleanlab': cleanlab_model(model_class, model_params, pulearning),\n",
    "                'micromodels_majority': micromodels(model_class, pulearning=pulearning,\n",
    "                                                    num_clfs=8, threshold=0.5, params=model_params),\n",
    "                'micromodels_consensus': micromodels(model_class, pulearning=pulearning,\n",
    "                                                     num_clfs=8, threshold=1, params=model_params),\n",
    "\n",
    "            }\n",
    "            print('generating datasets')\n",
    "            # preparing a dataset has some overhead, we want to do this five times for each dataset/noise level\n",
    "            # we will save a little bit of time by doing this in advance and using same set of five\n",
    "            # for each cleaning method\n",
    "            datasets = [prepare_noisy_dataset(key, noise_type, noise_amount, split=1, target_encoding=target_encoding) \n",
    "                        for i in range(num_samples)]\n",
    "            \n",
    "            # now for each cleaning method, train a \"clean\" model on noisy training data, then determine\n",
    "            # auc on clean test data and record the results. Do this five times for each cleaning method\n",
    "            # to determine mean/std. dev\n",
    "            for clf_type in clfs:\n",
    "                print(f\"testing {clf_type}\")\n",
    "                auc = []\n",
    "                try:\n",
    "                    for i in range(num_samples):\n",
    "                        # grab the dataset we need for this run and extract metadata and subsets\n",
    "                        dataset = datasets[i]\n",
    "                        features, cat_features, label = dataset['features'], dataset['cat_features'], dataset['label']\n",
    "                        train, test = dataset['train'], dataset['test']\n",
    "                        X_tr, y_tr = train[features], train[label].values.reshape(-1)\n",
    "                        X_ts, y_ts = test[features], test[label].values.reshape(-1)\n",
    "                        clf = clfs[clf_type]\n",
    "                        # fit the \"clean\" classifier on noisy training data\n",
    "                        clf.fit(X_tr, y_tr)\n",
    "                        # make predictions on clean test data and calculate AUC\n",
    "                        y_pred = clf.predict_proba(X_ts)[:, 1]\n",
    "                        auc.append(roc_auc_score(y_ts, y_pred))\n",
    "                        print(f\"{clf_type} auc: {auc}\", end=\"\\r\", flush=True)\n",
    "                    # store mean/std. dev for this run in the results dict\n",
    "                    results[key][noise_amount][clf_type] = (np.mean(auc), np.std(auc), auc)\n",
    "                    print('\\n{} auc: {:.2f} ± {:.4f}\\n'.format(clf_type,\n",
    "                                                               *results[key][noise_amount][clf_type][:2]))\n",
    "                # if this run failed for some reason, handle it gracefully\n",
    "                except Exception as e:\n",
    "                    results[key][noise_amount][clf_type] = (0, 0, [0] * num_samples)\n",
    "                    print(e)\n",
    "    \n",
    "    # if we are saving intermediate results to disk, do so now\n",
    "    if save_results:\n",
    "        with open(full_result_path, 'wb') as results_file:\n",
    "            pickle.dump(results[key], results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a4509",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a couple of helper functions to analyze/summarize results\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def record_places(places, scores):\n",
    "    scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    last_score, last_stddev, last_placement = (2, 0, 1)\n",
    "    for i, clf in enumerate(scores.keys()): \n",
    "        if scores[clf][0] + scores[clf][1] >= last_score:\n",
    "            placement = last_placement                          \n",
    "        else:\n",
    "            placement = i+1\n",
    "            last_score, last_stddev = scores[clf]            \n",
    "            last_placement = i+1\n",
    "        places[clf][placement] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa49c8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create dataframe of results for each experiment, also process results into dict for keeping track of \n",
    "# 1st/2nd/etc. place, as well as a dict for plotting later\n",
    "\n",
    "places = {clf:{p:0 for p in range(1,len(clf_types)+1)} for clf in clf_types}\n",
    "plots = {key:{clf:[[],[]] for clf in clf_types} for key in keys}\n",
    "        \n",
    "for key in results.keys():\n",
    "    print(f\"\\n =={key}==\\n\")\n",
    "    rows = pd.Index([clf_type for clf_type in clf_types])\n",
    "    columns = pd.MultiIndex.from_product([noise_amounts, ['mean','std_dev']], names=['type 2 noise', 'auc'])\n",
    "    df = pd.DataFrame(index=rows, columns=columns)\n",
    "    \n",
    "    for noise_amount in noise_amounts:\n",
    "        scores = {}\n",
    "        for clf_type in clf_types:\n",
    "            auc = results[key][noise_amount][clf_type]  \n",
    "            df.loc[clf_type, (noise_amount, 'mean')] = auc[0] \n",
    "            df.loc[clf_type, (noise_amount, 'std_dev')] = auc[1]\n",
    "            scores[clf_type] = (auc[0], auc[1])\n",
    "\n",
    "            plots[key][clf_type][0].append(noise_amount)\n",
    "            plots[key][clf_type][1].append(auc[0])\n",
    "        record_places(places, scores)\n",
    "    display(df.style.set_caption(f\"{key}\")\n",
    "            .format({(n,'mean'): \"{:.2f}\" for n in noise_amounts})\n",
    "            .format({(n,'std_dev'): \"{:.4f}\" for n in noise_amounts})\n",
    "            .apply(highlight_max, props='font-weight:bold;background-color:lightblue', axis=0,\n",
    "                  subset=[[n,'mean'] for n in noise_amounts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce \"race results\" (i.e. how many first place, second place, etc. finishes)\n",
    "\n",
    "race_results = pd.DataFrame.from_dict(places).rename(index=lambda x : humanize.ordinal(x))\n",
    "race_results['totals'] = race_results.sum(axis=1)\n",
    "display(race_results)\n",
    "print(race_results.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602877ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# finally, we can plot the results of individual experiments\n",
    "\n",
    "colors = ['black','purple','green','red','orange']\n",
    "linestyles = ['-','--',':']\n",
    "ylims = {\n",
    "    'boundary-consistent': {\n",
    "        'ieeecis':[0.5,0.9],\n",
    "        'sparknov':[0.5,1],\n",
    "        'ccfraud':[0.25,1],\n",
    "        'fraudecom':[0.48,0.52],\n",
    "        'fakejob':[0.5,1],\n",
    "        'vehicleloan':[0.57,0.66],\n",
    "        'twitterbot':[0.7,0.95]\n",
    "    },\n",
    "    'class-conditional': {\n",
    "        'ieeecis':[0.7,0.9],\n",
    "        'sparknov':[0.7,1],\n",
    "        'ccfraud':[0.8,1],\n",
    "        'fraudecom':[0.48,0.52],\n",
    "        'fakejob':[0.7,1],\n",
    "        'vehicleloan':[0.5,0.7],\n",
    "        'twitterbot':[0.8,0.95]\n",
    "    }\n",
    "}\n",
    "\n",
    "x_labels = {\n",
    "    'boundary-consistent':'Boundary-Consistent Noise Level',\n",
    "    'class-conditional':'Class-Conditional Type 2 Noise Level'\n",
    "}\n",
    "\n",
    "legends = {\n",
    "    'boundary-consistent':'Cleaning Method',\n",
    "    'class-conditional':'Type 1 Noise, Cleaning Method'\n",
    "}\n",
    "def fix_failures(x):\n",
    "    if x == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def labels(noise_type, noise_amount, clf_type):\n",
    "    if noise_type == 'boundary-consistent':\n",
    "        return '{}'.format(clf_type)\n",
    "    elif noise_type == 'class-conditional':\n",
    "        return '{}, {}'.format(noise_amount, clf_type)\n",
    "\n",
    "for key in results.keys():\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    for c, clf_type in enumerate(clf_types):\n",
    "        a = plots[key][clf_type]\n",
    "        plt.plot(a[0],[fix_failures(c) for c in a[1]],\n",
    "                 label=labels(noise_type, noise_amount, clf_type),\n",
    "                 color=colors[c],\n",
    "                 linestyle=linestyles[0])\n",
    "    plt.title(key)\n",
    "    plt.xlabel(x_labels[noise_type])\n",
    "    plt.ylabel('Test AUC')\n",
    "    plt.ylim(ylims[noise_type][key])\n",
    "    plt.legend(title=legends[noise_type])\n",
    "    plt.savefig(f\"./figures/label_noise_{key}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891c49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
